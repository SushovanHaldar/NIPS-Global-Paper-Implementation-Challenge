{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(8)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(80)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(800)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten, Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import math\n",
    "from scipy.stats import binom\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_CIFAR10:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_classes = 10\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [32,32,3]\n",
    "        self.batch_size = 128\n",
    "        self.epoches = 250\n",
    "        self.learning_rate = 0.1\n",
    "        self.lr_decay = 1e-6\n",
    "    \n",
    "    # Function to create dataset for training and validation of model\n",
    "    def create_dataset(self): \n",
    "        \n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # Create Train and Test datasets:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        \n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        \n",
    "        # Normalize the data\n",
    "        x_train, x_test = self.normalize(x_train, x_test)\n",
    "        \n",
    "        # Create one-hot encodings\n",
    "        y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    # Function to normalize train and validation datasets\n",
    "    def normalize(self,X_train,X_test): \n",
    "        \n",
    "        # Compute Mean\n",
    "        mean = np.mean(X_train,axis=(0, 1, 2, 3))\n",
    "        \n",
    "        # Compute Standard Deviation\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3)) \n",
    "        \n",
    "        # Normalize the data\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        \n",
    "        return X_train, X_test\n",
    "        \n",
    "    # Function to build the model   \n",
    "    def buildmodel(self): \n",
    "        \n",
    "        weight_decay = self.weight_decay\n",
    "        num_classes = self.num_classes\n",
    "        x_shape = self.x_shape\n",
    "        \n",
    "        model = Sequential()\n",
    "    \n",
    "        # First group of convolutional layer\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                         input_shape = x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Second group of convolutional layer\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Third group of convolutional layer\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Fourth group of convolutional layer\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Fifth group of convolutional layer\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Two Fully connected layer\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(100, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Function to train the model\n",
    "    def model_train(self, model, x_train, y_train, x_test, y_test, weights):\n",
    "        \n",
    "        if weights: # If model weights are already avaialble\n",
    "            model.load_weights('cifar10_vgg16.h5')\n",
    "        else:\n",
    "\n",
    "            # Training parameters\n",
    "            batch_size = self.batch_size\n",
    "            number_epoches = self.epoches\n",
    "            learning_rate = self.learning_rate\n",
    "            lr_decay = self.lr_decay\n",
    "\n",
    "            # Data augmentation\n",
    "            dataaugmentation = ImageDataGenerator(\n",
    "                                    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                                    samplewise_center=False,  # set each sample mean to 0\n",
    "                                    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                                    samplewise_std_normalization=False,  # divide each input by its std\n",
    "                                    zca_whitening=False,  # apply ZCA whitening\n",
    "                                    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                                    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                                    horizontal_flip=True,  # randomly flip images\n",
    "                                    vertical_flip=False)  # randomly flip images\n",
    "        \n",
    "            dataaugmentation.fit(x_train)\n",
    "\n",
    "            # Optimization details\n",
    "            sgd = optimizers.SGD(lr=0.0, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "            # Function to reduce learning rate by half after every 25 epochs\n",
    "            def step_decay(epoch):\n",
    "        \n",
    "                # LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)\n",
    "        \n",
    "                initial_lrate = 0.1\n",
    "                drop = 0.5\n",
    "                epochs_drop = 25.0\n",
    "                lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "                return lrate\n",
    "\n",
    "            # Callback for learning rate schedule\n",
    "            lrate = LearningRateScheduler(step_decay)\n",
    "            callbacks_list = [lrate]\n",
    "\n",
    "            # spe = Steps per epoch\n",
    "            spe = x_train.shape[0] // batch_size\n",
    "        \n",
    "            # Fit the model\n",
    "            model.fit_generator(dataaugmentation.flow(x_train, y_train,\n",
    "                                                 batch_size = batch_size),\n",
    "                                    steps_per_epoch = spe, callbacks=callbacks_list,\n",
    "                                    epochs = number_epoches,\n",
    "                                    validation_data = (x_test, y_test))\n",
    "        \n",
    "            # Save model weights\n",
    "            model.save_weights('cifar10_vgg16.h5')\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class object\n",
    "model_cifar10 = VGG16_CIFAR10()\n",
    "\n",
    "# Training and validation datasets\n",
    "x_train, y_train, x_test, y_test = model_cifar10.create_dataset()\n",
    "\n",
    "# Create model\n",
    "model = model_cifar10.buildmodel()\n",
    "\n",
    "# Train the model\n",
    "model = model_cifar10.model_train(model, x_train, y_train, x_test, y_test, weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "predict_test = model.predict(x_test)\n",
    "\n",
    "# Get highest probability on test set\n",
    "predict_test_prob = np.max(predict_test,1)\n",
    "\n",
    "# 0 for correct prediction and 1 for wrong prediction\n",
    "residuals = (np.argmax(predict_test,1) != np.argmax(y_test,1))\n",
    "\n",
    "# Loss computation\n",
    "loss = (-1)*((residuals*np.log10(predict_test_prob)) + ((1-residuals)*np.log(1-predict_test_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking validation accuracy is matching with our calculations\n",
    "Accuracy = ((10000 - sum(residuals))/10000)*100\n",
    "\n",
    "print(\"Accuracy is: \", Accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the validation dataset for training and testing SGR algorithm\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=8)\n",
    "\n",
    "for train_index, test_index in sss.split(x_test, y_test):\n",
    "    sgr_x_train, sgr_x_test = x_test[train_index], x_test[test_index]\n",
    "    sgr_y_train, sgr_y_test = y_test[train_index], y_test[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on SGR train set\n",
    "predict_sgr_train = model.predict(sgr_x_train)\n",
    "\n",
    "# Get highest probability on SGR train set\n",
    "predict_sgr_train_prob = np.max(predict_sgr_train,1)\n",
    "\n",
    "# 0 for wrong prediction and 1 for correct prediction for SGR train set\n",
    "residuals_sgr_train = (np.argmax(predict_sgr_train,1)!=np.argmax(sgr_y_train,1))\n",
    "\n",
    "# Loss computation on SGR train set\n",
    "loss_sgr_train = (-1)*((residuals_sgr_train*np.log10(predict_sgr_train_prob)) + ((1-residuals_sgr_train)*np.log(1-predict_sgr_train_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on SGR test set\n",
    "predict_sgr_test = model.predict(sgr_x_test)\n",
    "\n",
    "# Get highest probability on SGR test set\n",
    "predict_sgr_test_prob = np.max(predict_sgr_test,1)\n",
    "\n",
    "# 0 for wrong prediction and 1 for correct prediction for SGR test set\n",
    "residuals_sgr_test = (np.argmax(predict_sgr_test,1)!=np.argmax(sgr_y_test,1))\n",
    "\n",
    "# Loss computation on SGR test set\n",
    "loss_sgr_test = (-1)*((residuals_sgr_test*np.log10(predict_sgr_test_prob)) + ((1-residuals_sgr_test)*np.log(1-predict_sgr_test_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bound(delta, m, risk):\n",
    "        \n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        x = risk         # Lower bound\n",
    "        z = 1            # Upper bound\n",
    "        y = (x + z)/2    # mid point\n",
    "        \n",
    "        epsilonhat  = (-1*delta) + scipy.stats.binom.cdf(int(m*risk), m, y)\n",
    "        \n",
    "        while abs(epsilonhat)>epsilon:\n",
    "            if epsilonhat>0:\n",
    "                x = y\n",
    "            else:\n",
    "                z = y\n",
    "                \n",
    "            y = (x + z)/2\n",
    "            #print(\"x\", x)\n",
    "            #print(\"y\", y)\n",
    "            epsilonhat  = (-1*delta) + scipy.stats.binom.cdf(int(m*risk), m, y)\n",
    "            #print(epsilonhat)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGR(targetrisk, delta, predict_sgr_train_prob, predict_sgr_test_prob, residuals_sgr_train, residuals_sgr_test):\n",
    "\n",
    "        # Number of training samples for SGR algorithm\n",
    "        m = len(residuals_sgr_train)\n",
    "\n",
    "        # Sort the probabilities\n",
    "        probs_idx_sorted = np.argsort(predict_sgr_train_prob)\n",
    "\n",
    "        zmin = 0\n",
    "        zmax = m-1\n",
    "        deltahat = delta/math.ceil(math.log2(m))\n",
    "\n",
    "        for i in range(math.ceil(math.log2(m) + 1)):\n",
    "            \n",
    "            #print(\"iteration\", i)\n",
    "\n",
    "            mid = math.ceil((zmin+zmax)/2)\n",
    "\n",
    "            mi = len(residuals_sgr_train[probs_idx_sorted[mid:]])\n",
    "            theta = predict_sgr_train_prob[probs_idx_sorted[mid]]\n",
    "            trainrisk = sum(residuals_sgr_train[probs_idx_sorted[mid:]])/mi\n",
    "            \n",
    "            \n",
    "            testrisk = (sum(residuals_sgr_test[predict_sgr_test_prob>=theta]))/(len(residuals_sgr_test[predict_sgr_test_prob>=theta])+1)\n",
    "            testcoverage = (len(residuals_sgr_test[predict_sgr_test_prob>=theta]))/(len(predict_sgr_test_prob))\n",
    "                \n",
    "            \n",
    "            bound = calculate_bound(deltahat, mi, trainrisk)\n",
    "            traincoverage = mi/m\n",
    "            \n",
    "            if bound>targetrisk:\n",
    "                zmin = mid\n",
    "            else:\n",
    "                zmax = mid\n",
    "\n",
    "        return targetrisk, trainrisk, traincoverage, testrisk, testcoverage, bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confidence level parameter delta\n",
    "delta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_risk = []\n",
    "train_risk = []\n",
    "train_coverage = []\n",
    "test_risk = []\n",
    "test_coverage = []\n",
    "risk_bound = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different desired risk values\n",
    "rstar = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the SGR algorithm for different desired risk values\n",
    "\n",
    "for i in range(len(rstar)):\n",
    "    \n",
    "    # For desired risk 0.01\n",
    "    desiredrisk, trainrisk, traincov, testrisk, testcov, riskbound = SGR(rstar[i],delta, predict_sgr_train_prob, predict_sgr_test_prob, residuals_sgr_train, residuals_sgr_test)\n",
    "\n",
    "    # Append the values to the list\n",
    "    desired_risk.append(desiredrisk)\n",
    "    train_risk.append(trainrisk)\n",
    "    train_coverage.append(traincov)\n",
    "    test_risk.append(testrisk)\n",
    "    test_coverage.append(testcov)\n",
    "    risk_bound.append(riskbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = [('Desired Risk', desired_risk) ,\n",
    "          ('Train Risk', train_risk),\n",
    "          ('Train Coverage', train_coverage),\n",
    "          ('Test Risk', test_risk),\n",
    "          ('Test Coverage', test_coverage),\n",
    "          ('Risk bound', risk_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.DataFrame.from_items(Result)\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
